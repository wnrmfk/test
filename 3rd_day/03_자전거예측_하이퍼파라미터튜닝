
# coding: utf-8

# # LA Bike Demand

# - Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.
# 
# - The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.
# 
# 
# 
# - 캐글에 들어가서 train데이터와 test데이터 다운받기:
# 
# https://www.kaggle.com/c/bike-sharing-demand
# 
# 

# In[2]:


import pandas as pd


# ## Load Dataset

# In[3]:


train = pd.read_csv("data/biketrain.csv", parse_dates=["datetime"])

print(train.shape)
train.head()


# ## Explore

# In[4]:


get_ipython().run_line_magic('matplotlib', 'inline')
import seaborn as sns
import matplotlib.pyplot as plt


# ### Explore datetime
# - 날짜데이터 파싱하기

# In[5]:


train["datetime-year"] = train["datetime"].dt.year
train["datetime-month"] = train["datetime"].dt.month
train["datetime-day"] = train["datetime"].dt.day
train["datetime-hour"] = train["datetime"].dt.hour
train["datetime-minute"] = train["datetime"].dt.minute
train["datetime-second"] = train["datetime"].dt.second

print(train.shape)
train[["datetime", "datetime-year", "datetime-month", "datetime-day", "datetime-hour", "datetime-minute", "datetime-second"]].head()


# - 년도, 월, 날짜, 시간에 따라 자전거 대여댓수의 차이가 클까?

# In[6]:


figure, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(nrows=2, ncols=3)
figure.set_size_inches(18, 8)

sns.barplot(data=train, x="datetime-year", y="count", ax=ax1)
sns.barplot(data=train, x="datetime-month", y="count", ax=ax2)
sns.barplot(data=train, x="datetime-day", y="count", ax=ax3)
sns.barplot(data=train, x="datetime-hour", y="count", ax=ax4)
sns.barplot(data=train, x="datetime-minute", y="count", ax=ax5)
sns.barplot(data=train, x="datetime-second", y="count", ax=ax6)


# ** Lesson Learned **
#   * **datetime-minute**와 **datetime-second**는 현재 기록되고 있지 않다. 그러므로 사용할 필요가 없다.
#   * train.csv와 test.csv는 **datetime-day**를 기준으로 나뉘어져 있다. 그러므로 **datetime-day**를 feature로 사용해서는 안 된다.

# ### Explore hour - workingday
# - 그렇다면 주중과 주말의 대여수요의 차이는?

# In[7]:


figure, (ax1, ax2) = plt.subplots(nrows=2, ncols=1)
figure.set_size_inches(18, 8)

sns.pointplot(data=train, x="datetime-hour", y="count", ax=ax1)
sns.pointplot(data=train, x="datetime-hour", y="count", hue="workingday", ax=ax2)


# ** Lesson Learned **
#   * 주중(workingday==1)에는 출근 시간과 퇴근 시간에 자전거를 많이 대여한다.
#   * 주말(workingday==0)에는 오후 시간에 자전거를 많이 대여한다.
#   * 주중(월,화,수,목,금)이 주말(토,일)보다 많기 때문에, 두 개를 나눠서 보지 않으면 주말의 특성을 파악할 수 없다.

# ### Explore hour - dayofweek
# - 요일에 따라서도 달라지지 않을까?

# In[8]:


train["datetime-dayofweek"] = train["datetime"].dt.dayofweek

print(train.shape)
train[["datetime", "datetime-dayofweek"]].head()


# In[9]:


figure, (ax1, ax2) = plt.subplots(nrows=2, ncols=1)
figure.set_size_inches(18, 8)

sns.pointplot(data=train, x="datetime-hour", y="count", hue="workingday", ax=ax1)
sns.pointplot(data=train, x="datetime-hour", y="count", hue="datetime-dayofweek", ax=ax2)


# ** Lesson Learned **
#   * 금요일(workingday==4)는 주중이지만, 아주 약간 주말의 특성을 반영하고 있다.
#   * 비슷하게 월요일(workingday==0)도 아주 약간 주말의 특성을 반영하고 있다.
#   * 사람들이 휴가를 월요일과 금요일에 사용하기 때문이라고 추측할 수 있다.

# ### Concatenate year and month
# - 그런데 알고보니, 년도 달을 합치면 또다른 insight를 얻게 된다

# In[10]:


def concatenate_year_month(datetime):
    return "{0}-{1}".format(datetime.year, datetime.month)

train["datetime-year_month"] = train["datetime"].apply(concatenate_year_month)

print(train.shape)
train[["datetime", "datetime-year_month"]].head()


# In[11]:


figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)
figure.set_size_inches(18, 4)

sns.barplot(data=train, x="datetime-year", y="count", ax=ax1)
sns.barplot(data=train, x="datetime-month", y="count", ax=ax2)


figure, ax3 = plt.subplots(nrows=1, ncols=1)
figure.set_size_inches(18, 4)

sns.barplot(data=train, x="datetime-year_month", y="count", ax=ax3)


# ** Lesson Learned **
#   * 2011년 12월과 2012년 1월의 자전거 대여량을 비슷하지만, 두 개를 따로 놓고 보면 이를 알 수 없다.
#   * 2011년에는 8월부터 대여량이 감소하고, 2012년에는 7월부터 대여량이 감소한다. 마찬가지로 따로 놓고 보면 이를 알 수 없다.

# ## Reload Dataset
# - 시각화를 통해 중요한 feature를 파악했으니, 이제 본격적으로 머신러닝을 통해 예측해보자

# In[12]:


train = pd.read_csv("data/biketrain.csv", parse_dates=["datetime"])

print(train.shape)
train.head()


# In[14]:


test = pd.read_csv("data/biketest.csv", parse_dates=["datetime"])

print(test.shape)
test.head()


# ## Preprocessing

# ### Parse datetime

# In[15]:


train["datetime-year"] = train["datetime"].dt.year
train["datetime-month"] = train["datetime"].dt.month
train["datetime-day"] = train["datetime"].dt.day
train["datetime-hour"] = train["datetime"].dt.hour
train["datetime-minute"] = train["datetime"].dt.minute
train["datetime-second"] = train["datetime"].dt.second
train["datetime-dayofweek"] = train["datetime"].dt.dayofweek

print(train.shape)
train[["datetime", "datetime-year", "datetime-month", "datetime-day", "datetime-hour", "datetime-minute", "datetime-second", "datetime-dayofweek"]].head()


# In[16]:


test["datetime-year"] = test["datetime"].dt.year
test["datetime-month"] = test["datetime"].dt.month
test["datetime-day"] = test["datetime"].dt.day
test["datetime-hour"] = test["datetime"].dt.hour
test["datetime-minute"] = test["datetime"].dt.minute
test["datetime-second"] = test["datetime"].dt.second
test["datetime-dayofweek"] = test["datetime"].dt.dayofweek

print(test.shape)
test[["datetime", "datetime-year", "datetime-month", "datetime-day", "datetime-hour", "datetime-minute", "datetime-second", "datetime-dayofweek"]].head()


# ## Train
# - 학습시킬때 넣을 특성을 잘 골라서 넣는것이 중요! 모든 컬럼을 다 넣는다고 절대 좋은 모델을 만들수가 없다!

# In[17]:


feature_names = ["season", "holiday", "workingday", "weather",
                 "temp", "atemp", "humidity", "windspeed",
                 "datetime-year", "datetime-hour", "datetime-dayofweek"]

feature_names


# - X_train, y_train, X_test, 그렇다면 y_test는? 우리가 맞춰야할 답. 즉 test데이터의 자전거 대여수임. 

# In[18]:


X_train = train[feature_names]

print(X_train.shape)
X_train.head()


# In[19]:


X_test = test[feature_names]

print(X_test.shape)
X_test.head()


# In[20]:


label_name = "count"

y_train = train[label_name]

print(y_train.shape)
y_train.head()


# In[21]:


import numpy as np
y_train = np.log(y_train + 1)

print(y_train.shape)
y_train.head()


# - 머신러닝 지도학습 알고리즘의 종류, 회귀문제를 풀수 있는 알고리즘들에 대한 소개
# - 그중에서도 Decision Tree계열의 Random Froest의 장점!

# In[23]:


from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100,
                                  max_depth=7,
                                  max_features=0.7,
                                  random_state=37,
                                  n_jobs=-1)


# - 파라미터들을 어떻게 설정해줘야 할까? 가장 최적의 파라미터 값을 찾아내야함!

# In[24]:


model.fit(X_train, y_train)
predictions = model.predict(X_test)

predictions = np.exp(predictions) - 1

print(predictions.shape)
predictions


# In[26]:


submission = pd.read_csv("data/sampleSubmission.csv")

submission["count"] = predictions

print(submission.shape)
submission.head()
submission.to_csv("baseline-script.csv", index=False)


# - 파라미터를 최적화시켜서 점수를 올려보자!

# ## Hyperparameter Tuning

# ### Case 1 - Grid Search

# In[21]:


vocabulary = {'apple': "사과", "banana": "바나나"}
# vocabulary["banana"]


# In[ ]:


from sklearn.ensemble import RandomForestRegressor
from sklearn.cross_validation import cross_val_score

n_estimators = 300

max_depth_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]
max_features_list = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
hyperparameters_list = []
for max_depth in max_depth_list:
    for max_features in max_features_list:
        model = RandomForestRegressor(n_estimators=n_estimators,
                                      max_depth=max_depth,
                                      max_features=max_features,
                                      random_state=37,
                                      n_jobs=-1)
        score = cross_val_score(model, X_train, y_train, cv=20,                                 scoring=rmsle_scorer).mean()
        hyperparameters_list.append({
            'score': score,
            'n_estimators': n_estimators,
            'max_depth': max_depth,
            'max_features': max_features,
        })

        print("Score = {0:.5f}".format(score))

hyperparameters_list


# In[ ]:


hyperparameters_list = pd.DataFrame.from_dict(hyperparameters_list)
hyperparameters_list = hyperparameters_list.sort_values(by="score")

print(hyperparameters_list.shape)
hyperparameters_list.head()


# ### Case 2 - Random Search

# In[ ]:


np.random.randint(low=2, high=100)


# In[ ]:


np.random.uniform(low=0.1, high=1.0)


# In[ ]:


import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.cross_validation import cross_val_score

hyperparameters_list = []

n_estimators = 10
num_epoch = 10

for epoch in range(num_epoch):
    max_depth = np.random.randint(low=2, high=100)
    max_features = np.random.uniform(low=0.1, high=1.0)

    model = RandomForestRegressor(n_estimators=n_estimators,
                                  max_depth=max_depth,
                                  max_features=max_features,
                                  random_state=37,
                                  n_jobs=-1)

    score = cross_val_score(model, X_train, y_train, cv=20,                             scoring=rmsle_scorer).mean()

    hyperparameters_list.append({
        'score': score,
        'n_estimators': n_estimators,
        'max_depth': max_depth,
        'max_features': max_features,
    })

    print("Score = {0:.5f}".format(score))

hyperparameters_list = pd.DataFrame.from_dict(hyperparameters_list)
hyperparameters_list = hyperparameters_list.sort_values(by="score")

print(hyperparameters_list.shape)
hyperparameters_list.head()


# ### Case 2 - Finer Search

# In[ ]:


import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.cross_validation import cross_val_score

hyperparameters_list = []

# n_estimators = 300
# num_epoch = 100

n_estimators = 10
num_epoch = 10

for epoch in range(num_epoch):
    max_depth = np.random.randint(low=10, high=70)
    max_features = np.random.uniform(low=0.4, high=1.0)

    model = RandomForestRegressor(n_estimators=n_estimators,
                                  max_depth=max_depth,
                                  max_features=max_features,
                                  random_state=37,
                                  n_jobs=-1)

    score = cross_val_score(model, X_train, y_train, cv=20,                             scoring=rmsle_scorer).mean()

    hyperparameters_list.append({
        'score': score,
        'n_estimators': n_estimators,
        'max_depth': max_depth,
        'max_features': max_features,
    })

    print("Score = {0:.5f}".format(score))

hyperparameters_list = pd.DataFrame.from_dict(hyperparameters_list)
hyperparameters_list = hyperparameters_list.sort_values(by="score")

print(hyperparameters_list.shape)
hyperparameters_list.head()


# In[ ]:


from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=3000,
                              max_depth=83,
                              max_features=0.851358,
                              random_state=37,
                              n_jobs=-1)
model


# ## Score :케글의 평가방식

# $$ \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 } $$

# - log를 사용하면 상대적으로 큰 값이 작게 느껴지게 하는 효과
# - 주중에 렌탈수가 적을때 500대 틀린 오차와 주말에 렌탈수가 많을때 500대 틀린건 다르게 평가해야함

# In[ ]:


import numpy as np
from sklearn.metrics import make_scorer

def rmsle(predict, actual):
    predict = np.array(predict)
    actual = np.array(actual)
    
#     log_predict = np.log(predict + 1)
#     log_actual = np.log(actual + 1)
    log_predict = predict + 1
    log_actual = actual + 1
    
    difference = log_predict - log_actual
    difference = np.square(difference)
    
    mean_difference = difference.mean()
    
    score = np.sqrt(mean_difference)
    
    return score

rmsle_scorer = make_scorer(rmsle)
rmsle_scorer


# In[ ]:


from sklearn.cross_validation import cross_val_score

score = cross_val_score(model, X_train, y_train, cv=20,                         scoring=rmsle_scorer).mean()

print("Score = {0:.5f}".format(score))


# ## Train

# In[ ]:


model.fit(X_train, y_train)


# In[ ]:



predictions = model.predict(X_test)

predictions = np.exp(predictions) - 1

print(predictions.shape)
predictions


# ## Submit

# In[ ]:


submission = pd.read_csv("sampleSubmission.csv")

submission["count"] = predictions

print(submission.shape)
submission.head()


# In[27]:


submission.to_csv("baseline-scriptV2.csv", index=False)


# In[ ]:


list(zip(feature_names, model.feature_importances_))

